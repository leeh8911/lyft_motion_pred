{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lyft: Understanding the data  and EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credits:\n",
    "\n",
    "**https://www.kaggle.com/t3nyks/lyft-working-with-map-api**<br>\n",
    "**https://www.kaggle.com/jpbremer/lyft-scene-visualisations**<br>\n",
    "**https://www.kaggle.com/pestipeti/pytorch-baseline-train**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This new Lyft competition is tasking us, the participants, to predict the motion of external cars, cyclists, pedestrians etc. to assist self-driving cars. This is a step ahead from last year's competition, where we were tasked with detecting three-dimensional objects, like stop signs, to teach AVs how to recognize these. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TIP: Use plt.imshow instead of IPython.display**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is apparently the **largest collection of traffic agent motion data.** The files are stored in the .zarr file format with Python, which we can easily load using the Level 5 Kit (l5kit for the pip package). Within our training ZARRs, we have the agents, the masks for agents, frames and scenes (which you might recollect from last year) and traffic light faces.\n",
    "\n",
    "The test ZARR however is almost practically the same format, but the only exclusion is that of the data masks. for the agents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get started with the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait! Before we directly get into the fancy visualization and all it entails, why not watch a short YouTube video to enlighten us on the subject of operating an autonomous vehicle?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<center><iframe width=\"700\" height=\"400\" src=\"https://www.youtube.com/embed/tlThdr3O5Qo?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe></center>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<center><iframe width=\"700\" height=\"400\" src=\"https://www.youtube.com/embed/tlThdr3O5Qo?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe></center>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like this car casually handles all the normal challenges a driver faces, and that too with remarkable accuracy. Over here, we are tasked with somethign to faciliate this sort of thing - **predicting the motion of extraneous vehicles and based on that, predicting the motion path of an AV.**  To predict the motion of these extraneous factors, there are many approaches which I shall discuss later, but for now let's dive in.\n",
    "\n",
    "Here's a brief FAQ section about the dataset and all it entails:\n",
    "\n",
    "**What is the structure of the dataset?**<br>\n",
    "The dataset is structured as follows:\n",
    "```\n",
    "aerial_map\n",
    "scenes\n",
    "semantic_map\n",
    "```\n",
    "\n",
    "where each scene contains roughly a minute or so of information about the motion of several extraneous vehicles and the corresponding movement of the AV.\n",
    "\n",
    "Under scenes, we have:\n",
    "```\n",
    "sample.zarr\n",
    "test.zarr\n",
    "train.zarr\n",
    "validate.zarr\n",
    "```\n",
    "\n",
    "Now this ZARR format is a little bit interesting, as I am willing to fathom a guess most of the participants have never worked with these. Fear not, for they are very much interoperable with NumPy and the Lyft Level 5 Kit also gives us easy ways to handle the processing of the data. Of course, there also might be a few ways to use a Pandas DataFrame in the process, which leads up a road for LightGBM.\n",
    "\n",
    "The train.zarr contains the agents, the mask for the agents, the frames, the scenes and the traffic light faces, which I'll go into more depth later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may now import the lyft level 5 kit, and all that comes with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './lyft-config-files/visualisation_config.yaml'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-bd178ac75c00>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"L5KIT_DATA_FOLDER\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"./lyft-motion-prediction-autonomous-vehicles\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# get config\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mcfg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_config_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./lyft-config-files/visualisation_config.yaml\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moffline\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_notebook_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconnected\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\lyft2\\lib\\site-packages\\l5kit\\configs\\config.py\u001b[0m in \u001b[0;36mload_config_data\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_config_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mcfg\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0myaml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLoader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0myaml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFullLoader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcfg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './lyft-config-files/visualisation_config.yaml'"
     ]
    }
   ],
   "source": [
    "import l5kit, os\n",
    "from l5kit.rasterization import build_rasterizer\n",
    "from l5kit.configs import load_config_data\n",
    "from l5kit.visualization import draw_trajectory, TARGET_POINTS_COLOR\n",
    "from l5kit.geometry import transform_points\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from l5kit.data import PERCEPTION_LABELS\n",
    "from prettytable import PrettyTable\n",
    "import matplotlib.pyplot as plt\n",
    "# set env variable for data\n",
    "os.environ[\"L5KIT_DATA_FOLDER\"] = \"da/lyft-motion-prediction-autonomous-vehicles\"\n",
    "# get config\n",
    "cfg = load_config_data(\"./lyft-config-files/visualisation_config.yaml\")\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's get a sense of the configuration data. This will include metadata pertaining to the agents, the total time, the frames-per-scene, the scene time and the frame frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from l5kit.data import ChunkedDataset, LocalDataManager\n",
    "from l5kit.dataset import EgoDataset, AgentDataset\n",
    "dm = LocalDataManager()\n",
    "dataset_path = dm.require(cfg[\"val_data_loader\"][\"key\"]);rasterizer = build_rasterizer(cfg, dm)\n",
    "zarr_dataset = ChunkedDataset(dataset_path)\n",
    "train_dataset_a = AgentDataset(cfg, zarr_dataset, rasterizer)\n",
    "zarr_dataset.open()\n",
    "print(zarr_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, however it's time for us to look at the scenes and analyze them in depth. Theoretically, we could create a nifty little data-loader to do some heavy lifting for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import display, clear_output\n",
    "import PIL\n",
    " \n",
    "cfg[\"raster_params\"][\"map_type\"] = \"py_semantic\"\n",
    "rast = build_rasterizer(cfg, dm)\n",
    "dataset = EgoDataset(cfg, zarr_dataset, rast)\n",
    "scene_idx = 2\n",
    "indexes = dataset.get_scene_indices(scene_idx)\n",
    "images = []\n",
    "\n",
    "for idx in indexes:\n",
    "    \n",
    "    data = dataset[idx]\n",
    "    im = data[\"image\"].transpose(1, 2, 0)\n",
    "    im = dataset.rasterizer.to_rgb(im)\n",
    "    target_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\n",
    "    center_in_pixels = np.asarray(cfg[\"raster_params\"][\"ego_center\"]) * cfg[\"raster_params\"][\"raster_size\"]\n",
    "    draw_trajectory(im, target_positions_pixels, data[\"target_yaws\"], TARGET_POINTS_COLOR)\n",
    "    clear_output(wait=True)\n",
    "    display(PIL.Image.fromarray(im[::-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, there's a lot of information in this one image. I'll try my best to point everything out, but do notify me if I make any errors. OK, let's get started with dissecting the image:\n",
    "+ We have an intersection of four roads over here.\n",
    "+ The green blob represents the AV's motion, and we would require to predict the movement of the AV in these traffic conditions as a sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't exactly know what other inferences we can make without more detail on this data, so let's try a satellite-format viewing of these images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import display, clear_output\n",
    "import PIL\n",
    " \n",
    "cfg[\"raster_params\"][\"map_type\"] = \"py_satellite\"\n",
    "rast = build_rasterizer(cfg, dm)\n",
    "dataset = EgoDataset(cfg, zarr_dataset, rast)\n",
    "scene_idx = 2\n",
    "indexes = dataset.get_scene_indices(scene_idx)\n",
    "images = []\n",
    "\n",
    "for idx in indexes:\n",
    "    \n",
    "    data = dataset[idx]\n",
    "    im = data[\"image\"].transpose(1, 2, 0)\n",
    "    im = dataset.rasterizer.to_rgb(im)\n",
    "    target_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\n",
    "    center_in_pixels = np.asarray(cfg[\"raster_params\"][\"ego_center\"]) * cfg[\"raster_params\"][\"raster_size\"]\n",
    "    draw_trajectory(im, target_positions_pixels, data[\"target_yaws\"], TARGET_POINTS_COLOR)\n",
    "    clear_output(wait=True)\n",
    "    display(PIL.Image.fromarray(im[::-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes! This allows for far more detail than a simple plot without detail. I'd haphazard an educated guess, and make the following inferences:\n",
    "+ Green still represents the autonomous vehicle (AV), and blue is primarily all the other cars/vehicles/exogenous factors we need to predict for.\n",
    "+ My hypothesis is that the blue represents the path the vehicle needs to go through.\n",
    "+ If we are able to accurately predict the path the vehicles go through, it will make it easier for an AV to compute its trajectory on the fly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to see how the whole charade of vehicles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "from IPython.display import HTML\n",
    "\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation, rc\n",
    "def animate_solution(images):\n",
    "\n",
    "    def animate(i):\n",
    "        im.set_data(images[i])\n",
    " \n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(images[0])\n",
    "    \n",
    "    return animation.FuncAnimation(fig, animate, frames=len(images), interval=60)\n",
    "cfg[\"raster_params\"][\"map_type\"] = \"py_satellite\"\n",
    "rast = build_rasterizer(cfg, dm)\n",
    "dataset = EgoDataset(cfg, zarr_dataset, rast)\n",
    "scene_idx = 34\n",
    "indexes = dataset.get_scene_indices(scene_idx)\n",
    "images = []\n",
    "\n",
    "for idx in indexes:\n",
    "    \n",
    "    data = dataset[idx]\n",
    "    im = data[\"image\"].transpose(1, 2, 0)\n",
    "    im = dataset.rasterizer.to_rgb(im)\n",
    "    target_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\n",
    "    center_in_pixels = np.asarray(cfg[\"raster_params\"][\"ego_center\"]) * cfg[\"raster_params\"][\"raster_size\"]\n",
    "    draw_trajectory(im, target_positions_pixels, data[\"target_yaws\"], TARGET_POINTS_COLOR)\n",
    "    clear_output(wait=True)\n",
    "    images.append(PIL.Image.fromarray(im[::-1]))\n",
    "anim = animate_solution(images)\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is a demonstration of the movement of the other vehicles and (in relation to the movement and placement of the other vehicles) the movement of the AV. The AV is currently taking only a straight path in its motion, and a straight path seems logical with the movement and placement of other vehicles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "from IPython.display import HTML\n",
    "\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation, rc\n",
    "def animate_solution(images):\n",
    "\n",
    "    def animate(i):\n",
    "        im.set_data(images[i])\n",
    " \n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(images[0])\n",
    "    \n",
    "    return animation.FuncAnimation(fig, animate, frames=len(images), interval=60)\n",
    "cfg[\"raster_params\"][\"map_type\"] = \"py_semantic\"\n",
    "rast = build_rasterizer(cfg, dm)\n",
    "dataset = EgoDataset(cfg, zarr_dataset, rast)\n",
    "scene_idx = 34\n",
    "indexes = dataset.get_scene_indices(scene_idx)\n",
    "images = []\n",
    "\n",
    "for idx in indexes:\n",
    "    \n",
    "    data = dataset[idx]\n",
    "    im = data[\"image\"].transpose(1, 2, 0)\n",
    "    im = dataset.rasterizer.to_rgb(im)\n",
    "    target_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\n",
    "    center_in_pixels = np.asarray(cfg[\"raster_params\"][\"ego_center\"]) * cfg[\"raster_params\"][\"raster_size\"]\n",
    "    draw_trajectory(im, target_positions_pixels, data[\"target_yaws\"], TARGET_POINTS_COLOR)\n",
    "    clear_output(wait=True)\n",
    "    images.append(PIL.Image.fromarray(im[::-1]))\n",
    "anim = animate_solution(images)\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're also able to take a more low-level move by using the semantic option in the Lyft level 5 kit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "import PIL\n",
    " \n",
    "cfg[\"raster_params\"][\"map_type\"] = \"py_semantic\"\n",
    "rast = build_rasterizer(cfg, dm)\n",
    "dataset = EgoDataset(cfg, zarr_dataset, rast)\n",
    "scene_idx = 34\n",
    "indexes = dataset.get_scene_indices(scene_idx)\n",
    "images = []\n",
    "\n",
    "for idx in indexes:\n",
    "    \n",
    "    data = dataset[idx]\n",
    "    im = data[\"image\"].transpose(1, 2, 0)\n",
    "    im = dataset.rasterizer.to_rgb(im)\n",
    "    target_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\n",
    "    center_in_pixels = np.asarray(cfg[\"raster_params\"][\"ego_center\"]) * cfg[\"raster_params\"][\"raster_size\"]\n",
    "    draw_trajectory(im, target_positions_pixels, data[\"target_yaws\"], TARGET_POINTS_COLOR)\n",
    "    clear_output(wait=True)\n",
    "    images.append(PIL.Image.fromarray(im[::-1]))\n",
    "    \n",
    "anim = animate_solution(images)\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The semantic view is good for a less clustered view but if we want a more detailed, more high-level overview of the data we should perhaps try to use the satellite view voer semantic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, how about from the agent perspective? This would be quite interesting to consider, as we're modeling from principally the agent perspective in most public notebooks so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import display, clear_output\n",
    "import PIL\n",
    " \n",
    "cfg[\"raster_params\"][\"map_type\"] = \"py_satellite\"\n",
    "rast = build_rasterizer(cfg, dm)\n",
    "dataset = AgentDataset(cfg, zarr_dataset, rast)\n",
    "scene_idx = 2\n",
    "indexes = dataset.get_scene_indices(scene_idx)\n",
    "images = []\n",
    "\n",
    "for idx in indexes:\n",
    "    \n",
    "    data = dataset[idx]\n",
    "    im = data[\"image\"].transpose(1, 2, 0)\n",
    "    im = dataset.rasterizer.to_rgb(im)\n",
    "    target_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\n",
    "    center_in_pixels = np.asarray(cfg[\"raster_params\"][\"ego_center\"]) * cfg[\"raster_params\"][\"raster_size\"]\n",
    "    draw_trajectory(im, target_positions_pixels, data[\"target_yaws\"], TARGET_POINTS_COLOR)\n",
    "    clear_output(wait=True)\n",
    "    display(PIL.Image.fromarray(im[::-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So yes, I probably should save these as a GIF to visualize the agent movements. Let's try a simpler form of this and use the semantic view for the agent dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import display, clear_output\n",
    "import PIL\n",
    " \n",
    "cfg[\"raster_params\"][\"map_type\"] = \"py_semantic\"\n",
    "rast = build_rasterizer(cfg, dm)\n",
    "dataset = AgentDataset(cfg, zarr_dataset, rast)\n",
    "scene_idx = 2\n",
    "indexes = dataset.get_scene_indices(scene_idx)\n",
    "images = []\n",
    "\n",
    "for idx in indexes:\n",
    "    \n",
    "    data = dataset[idx]\n",
    "    im = data[\"image\"].transpose(1, 2, 0)\n",
    "    im = dataset.rasterizer.to_rgb(im)\n",
    "    target_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\n",
    "    center_in_pixels = np.asarray(cfg[\"raster_params\"][\"ego_center\"]) * cfg[\"raster_params\"][\"raster_size\"]\n",
    "    draw_trajectory(im, target_positions_pixels, data[\"target_yaws\"], TARGET_POINTS_COLOR)\n",
    "    clear_output(wait=True)\n",
    "    display(PIL.Image.fromarray(im[::-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also take a general view of the street from a matplotlib-type perspective. I borrow this from [this wonderful notebook](https://www.kaggle.com/t3nyks/lyft-working-with-map-api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "from l5kit.data.map_api import MapAPI\n",
    "from l5kit.rasterization.rasterizer_builder import _load_metadata\n",
    "\n",
    "semantic_map_filepath = dm.require(cfg[\"raster_params\"][\"semantic_map_key\"])\n",
    "dataset_meta = _load_metadata(cfg[\"raster_params\"][\"dataset_meta_key\"], dm)\n",
    "world_to_ecef = np.array(dataset_meta[\"world_to_ecef\"], dtype=np.float64)\n",
    "\n",
    "map_api = MapAPI(semantic_map_filepath, world_to_ecef)\n",
    "MAP_LAYERS = [\"junction\", \"node\", \"segment\", \"lane\"]\n",
    "\n",
    "\n",
    "def element_of_type(elem, layer_name):\n",
    "    return elem.element.HasField(layer_name)\n",
    "\n",
    "\n",
    "def get_elements_from_layer(map_api, layer_name):\n",
    "    return [elem for elem in map_api.elements if element_of_type(elem, layer_name)]\n",
    "\n",
    "\n",
    "class MapRenderer:\n",
    "    \n",
    "    def __init__(self, map_api):\n",
    "        self._color_map = dict(drivable_area='#a6cee3',\n",
    "                               road_segment='#1f78b4',\n",
    "                               road_block='#b2df8a',\n",
    "                               lane='#474747')\n",
    "        self._map_api = map_api\n",
    "    \n",
    "    def render_layer(self, layer_name):\n",
    "        fig = plt.figure(figsize=(10, 10))\n",
    "        ax = fig.add_axes([0, 0, 1, 1])\n",
    "        \n",
    "    def render_lanes(self):\n",
    "        all_lanes = get_elements_from_layer(self._map_api, \"lane\")\n",
    "        fig = plt.figure(figsize=(10, 10))\n",
    "        ax = fig.add_axes([0, 0, 1, 1])\n",
    "        for lane in all_lanes:\n",
    "            self.render_lane(ax, lane)\n",
    "        return fig, ax\n",
    "        \n",
    "    def render_lane(self, ax, lane):\n",
    "        coords = self._map_api.get_lane_coords(MapAPI.id_as_str(lane.id))\n",
    "        self.render_boundary(ax, coords[\"xyz_left\"])\n",
    "        self.render_boundary(ax, coords[\"xyz_right\"])\n",
    "        \n",
    "    def render_boundary(self, ax, boundary):\n",
    "        xs = boundary[:, 0]\n",
    "        ys = boundary[:, 1] \n",
    "        ax.plot(xs, ys, color=self._color_map[\"lane\"], label=\"lane\")\n",
    "        \n",
    "        \n",
    "renderer = MapRenderer(map_api)\n",
    "fig, ax = renderer.render_lanes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uh it seems the rasterizer renders rather well the satellite and semantic views, and both in conjunction help one to get a good sense of the positioning of each vehicle in relation to the road. You can easily understand the placement and motion of the vehicles and highway layout in satellite by taking a good look at the semantic view too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def visualize_rgb_image(dataset, index, title=\"\", ax=None):\n",
    "    \"\"\"Visualizes Rasterizer's RGB image\"\"\"\n",
    "    data = dataset[index]\n",
    "    im = data[\"image\"].transpose(1, 2, 0)\n",
    "    im = dataset.rasterizer.to_rgb(im)\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    if title:\n",
    "        ax.set_title(title)\n",
    "    ax.imshow(im[::-1])\n",
    "# Prepare all rasterizer and EgoDataset for each rasterizer\n",
    "rasterizer_dict = {}\n",
    "dataset_dict = {}\n",
    "\n",
    "\n",
    "\n",
    "rasterizer_type_list = [\"py_satellite\", \"py_semantic\"]\n",
    "\n",
    "for i, key in enumerate(rasterizer_type_list):\n",
    "    # print(\"key\", key)\n",
    "    cfg[\"raster_params\"][\"map_type\"] = key\n",
    "    rasterizer_dict[key] = build_rasterizer(cfg, dm)\n",
    "    dataset_dict[key] = EgoDataset(cfg, zarr_dataset, rasterizer_dict[key])\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "for i, key in enumerate([\"semantic_debug\", \"box_debug\"]):\n",
    "    visualize_rgb_image(dataset_dict[key], index=0, title=f\"{key}: {type(rasterizer_dict[key]).__name__}\", ax=axes[i])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, box and stub will also give a good representaton of the data albeit with less low-level detail than the semantic view, seeing as the highways are not into much consideration here. The box view helps to just take a low-level look at the vehicles and their projected path whereas the stub view functions similarly to semantic. We can now proceed to taking a good look at the metadata provided by kkiller and potentially train a good model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can explore the images, we can also get a little down and dirty when it comes to the ZARR files. It's rather simple to use with the Python library for exploring them, especially the fact that it's NumPy interoperable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"scenes\", zarr_dataset.scenes)\n",
    "print(\"scenes[0]\", zarr_dataset.scenes[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, a gentle note that we can use the ChunkedDataset to generate CSV files of scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "scenes = zarr_dataset.scenes\n",
    "scenes_df = pd.DataFrame(scenes)\n",
    "scenes_df.columns = [\"data\"]; features = ['frame_index_interval', 'host', 'start_time', 'end_time']\n",
    "for i, feature in enumerate(features):\n",
    "    scenes_df[feature] = scenes_df['data'].apply(lambda x: x[i])\n",
    "scenes_df.drop(columns=[\"data\"],inplace=True)\n",
    "print(f\"scenes dataset: {scenes_df.shape}\")\n",
    "scenes_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But are we sure this is enough? Enough knowledge to satisfy us? No it's not. I will be using Kkiller's dataset for further tabular data exploration from henceforth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = pd.read_csv('../input/lyft-motion-prediction-autonomous-vehicles-as-csv/agents_0_10019001_10019001.csv')\n",
    "agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a literal wealth of information that we can use here to our benefits, including familiar features like:\n",
    "1. x, y,  and z coords\n",
    "2. yaw\n",
    "3. probabilites of other extraneous factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "colormap = plt.cm.magma\n",
    "cont_feats = [\"centroid_x\", \"centroid_y\", \"extent_x\", \"extent_y\", \"extent_z\", \"yaw\"]\n",
    "plt.figure(figsize=(16,12));\n",
    "plt.title('Pearson correlation of features', y=1.05, size=15);\n",
    "sns.heatmap(agents[cont_feats].corr(),linewidths=0.1,vmax=1.0, square=True, \n",
    "            cmap=colormap, linecolor='white', annot=True);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can extrapolate that the variables **centroid_x** and **centroid_y** have strongly negative correlations, and the strongest correlations are between **extent_z** and **extent_x** more than any other, coming in at 0.4. We can also try using an XGBoost/LightGBM model as kkiller has demonstrated in his brilliant kernel as an alternative approach to the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### centroid_x and centroid_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plot = sns.jointplot(x=agents['centroid_x'][:1000], y=agents['centroid_y'][:1000], kind='hexbin', color='blueviolet')\n",
    "plot.set_axis_labels('center_x', 'center_y', fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like the two centroids have a somewhat strongly negative correlation and seemingly similar variable distributions. It seems that as such there is a negative correlation between both the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 15));\n",
    "sns.distplot(agents['centroid_x'], color='steelblue');\n",
    "sns.distplot(agents['centroid_y'], color='purple');\n",
    "plt.title(\"Distributions of Centroid X and Y\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems the right-skewed distribution of centroid_x is considerably more extreme than that of centroid_y. The distributions both are very unsimilar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extent_x, extent_y and extent_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 15));\n",
    "sns.distplot(agents['extent_x'], color='steelblue');\n",
    "sns.distplot(agents['extent_y'], color='purple');\n",
    "\n",
    "plt.title(\"Distributions of Extents X and Y\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems both the distributions of extent X and extent Y are heavily right skewed, as is centroid X. However, I have left out extent Z is order for readability of the plot, let's look at it now.\n",
    "\n",
    "Try to smooth the data and get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 15));\n",
    "sns.distplot(agents['extent_z'], color='steelblue');\n",
    "\n",
    "plt.title(\"Distributions of Extents z\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we have a right-skewed distribution as is the same with all the `extent` variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### yaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 15));\n",
    "sns.distplot(agents['yaw'], color='steelblue');\n",
    "\n",
    "plt.title(\"Distributions of Extents z\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So yes it seems like this distribution has several \"protrusions\" as I shall call them. We can now move on to exploring the frames data to check how feasible it is for our tabular purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frms = pd.read_csv(\"../input/lyft-motion-prediction-autonomous-vehicles-as-csv/frames_0_124167_124167.csv\")\n",
    "frms.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here we have the ego rotations with regards to the centroids, which will be very interesting to consider. It seems like we will require to check multiple of these variables at once:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ego_rotatations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we have nine ego rotation columns corresponding to each. So I would want to do a quick check of the correlation of these variables before moving on to some more high-level analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "colormap = plt.cm.magma\n",
    "cont_feats = [\"ego_rotation_xx\", \"ego_rotation_xy\", \"ego_rotation_xz\", \"ego_rotation_yx\", \"ego_rotation_yy\", \"ego_rotation_yz\", \"ego_rotation_zx\", \"ego_rotation_zy\", \"ego_rotation_zz\"]\n",
    "plt.figure(figsize=(16,12));\n",
    "plt.title('Pearson correlation of features', y=1.05, size=15);\n",
    "sns.heatmap(frms[cont_feats].corr(),linewidths=0.1,vmax=1.0, square=True, \n",
    "            cmap=colormap, linecolor='white', annot=True);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to note from this correlation analysis:\n",
    "1. The rotation coordinates with `y` and `z` seem to be uncorrelated most of the time\n",
    "2. The coordinates which have `x` are correlated strongly with the z-dimensional rotation (could this be indicative of something? I very much think so)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### binary features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import numpy as np\n",
    "zero_count_list, one_count_list = [], []\n",
    "cols_list = [\"label_probabilities_PERCEPTION_LABEL_UNKNOWN\",\"label_probabilities_PERCEPTION_LABEL_CAR\",\"label_probabilities_PERCEPTION_LABEL_CYCLIST\",\"label_probabilities_PERCEPTION_LABEL_PEDESTRIAN\"]\n",
    "for col in cols_list:\n",
    "    zero_count_list.append((agents[col]==0).sum())\n",
    "    one_count_list.append((agents[col]==1).sum())\n",
    "fig = px.sunburst(data_frame=agents.head(1000),\n",
    "                  path=cols_list,\n",
    "                  color='label_probabilities_PERCEPTION_LABEL_UNKNOWN',\n",
    "                  maxdepth=-1,\n",
    "                  title='Labels of binary features')\n",
    "\n",
    "fig.update_layout(margin=dict(t=0, l=0, r=0, b=0))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline model (source: [here](https://github.com/lyft/l5kit/blob/master/examples/agent_motion_prediction/agent_motion_prediction.ipynb) and [here](https://www.kaggle.com/pestipeti/pytorch-baseline-inference))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/catalyst-team/catalyst-pics/master/pics/catalyst_logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is mainly me using the Lyft baseline model and training it, for the purpose of demonstrating how we can fit to a PyTorch model with the provided dataset format.\n",
    "\n",
    "Also using wonderful ML library catalyst, which helps your PyTorch training greatly. For now, let's get started on the modelling with some basic import setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torchvision.models.resnet import resnet18\n",
    "from catalyst import dl\n",
    "from catalyst.utils import metrics\n",
    "from torch.utils.data import DataLoader\n",
    "from catalyst.dl import utils\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're importing a typical CV stack coupled with our earlier l5kit imports. The things to note here are catalyst.dl and catalyst.utils, since both help your ML workflow infinitely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg2 = load_config_data(\"../input/lyft-config-files/agent_motion_config.yaml\")\n",
    "train_cfg = cfg2[\"train_data_loader\"]\n",
    "validation_cfg = cfg2[\"val_data_loader\"]\n",
    "# Rasterizer\n",
    "rasterizer = build_rasterizer(cfg2, dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is basically initializing the rasterization process for the training data which basically functions to pass the .zarr files to our model and make it a coherent data format easily modular with our PyTorch setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LyftModel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.backbone = resnet18(pretrained=True, progress=True)\n",
    "        \n",
    "        num_history_channels = (cfg[\"model_params\"][\"history_num_frames\"] + 1) * 2\n",
    "        num_in_channels = 3 + num_history_channels\n",
    "\n",
    "        self.backbone.conv1 = nn.Conv2d(\n",
    "            num_in_channels,\n",
    "            self.backbone.conv1.out_channels,\n",
    "            kernel_size=self.backbone.conv1.kernel_size,\n",
    "            stride=self.backbone.conv1.stride,\n",
    "            padding=self.backbone.conv1.padding,\n",
    "            bias=False,\n",
    "        )\n",
    "        \n",
    "        # This is 512 for resnet18 and resnet34;\n",
    "        # And it is 2048 for the other resnets\n",
    "        backbone_out_features = 512\n",
    "\n",
    "        # X, Y coords for the future positions (output shape: Bx50x2)\n",
    "        num_targets = 2 * cfg[\"model_params\"][\"future_num_frames\"]\n",
    "\n",
    "        # You can add more layers here.\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(in_features=backbone_out_features, out_features=4096),\n",
    "        )\n",
    "\n",
    "        self.logit = nn.Linear(4096, out_features=num_targets)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.backbone.conv1(x)\n",
    "        x = self.backbone.bn1(x)\n",
    "        x = self.backbone.relu(x)\n",
    "        x = self.backbone.maxpool(x)\n",
    "\n",
    "        x = self.backbone.layer1(x)\n",
    "        x = self.backbone.layer2(x)\n",
    "        x = self.backbone.layer3(x)\n",
    "        x = self.backbone.layer4(x)\n",
    "\n",
    "        x = self.backbone.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        x = self.head(x)\n",
    "        x = self.logit(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unhide the above cell if you want to see the model, it's basically Peter's original work. All I am doing here is modifying the training pipeline to be Catalyst-compatible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = LyftModel(cfg2)\n",
    "model.to(device)\n",
    "\n",
    "# Train dataset/dataloader\n",
    "train_zarr = ChunkedDataset(dm.require(train_cfg[\"key\"])).open()\n",
    "train_dataset = AgentDataset(cfg2, train_zarr, rasterizer)\n",
    "subset = torch.utils.data.Subset(train_dataset, range(0, 800))\n",
    "train_dataloader = DataLoader(subset,\n",
    "                              shuffle=train_cfg[\"shuffle\"],\n",
    "                              batch_size=train_cfg[\"batch_size\"],\n",
    "                              num_workers=train_cfg[\"num_workers\"])\n",
    "\n",
    "val_zarr = ChunkedDataset(dm.require(validation_cfg[\"key\"])).open()\n",
    "val_dataset = AgentDataset(cfg2, val_zarr, rasterizer)\n",
    "subset = torch.utils.data.Subset(val_dataset, range(0, 50))\n",
    "val_dataloader = DataLoader(subset,\n",
    "                              shuffle=validation_cfg[\"shuffle\"],\n",
    "                              batch_size=validation_cfg[\"batch_size\"],\n",
    "                              num_workers=validation_cfg[\"num_workers\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This takes a very small subset of the data that we have here (mainly because this training pipeline is purely for demonstration purposes with regards to Catalyst)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.02)\n",
    "\n",
    "\n",
    "loaders = {\n",
    "    \"train\": train_dataloader,\n",
    "    \"valid\": val_dataloader\n",
    "}\n",
    "\n",
    "class LyftRunner(dl.Runner):\n",
    "\n",
    "    def predict_batch(self, batch):\n",
    "        return self.model(batch[0].to(self.device).view(batch[0].size(0), -1))\n",
    "\n",
    "    def _handle_batch(self, batch):\n",
    "        x, y = batch['image'], batch['target_positions']\n",
    "        y_hat = self.model(x).reshape(y.shape)\n",
    "        target_availabilities = batch[\"target_availabilities\"].unsqueeze(-1)\n",
    "        criterion = torch.nn.MSELoss(reduction=\"none\")\n",
    "        loss = criterion(y_hat, y)\n",
    "        loss = loss * target_availabilities\n",
    "        loss = loss.mean()\n",
    "        self.batch_metrics.update(\n",
    "            {\"loss\": loss}\n",
    "        )\n",
    "\n",
    "        if self.is_train_loader:\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two things:\n",
    "+ Configuration of the data loaders\n",
    "+ Beginning the Catalyst process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "device = utils.get_device()\n",
    "runner = LyftRunner(device=device)\n",
    "# model training\n",
    "runner.train(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    loaders=loaders,\n",
    "    logdir=\"./logs\",\n",
    "    num_epochs=1,\n",
    "    verbose=True,\n",
    "    load_best_on_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://github.com/lyft/l5kit/blob/master/examples/visualisation/visualise_data.ipynb\n",
    "\n",
    "**WORK IN PROGRESS - MORE TO COME.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
